---
---
\usepackage{url}
@string{aps = {American Physical Society,}}

@misc{guo2024mechanisticunlearningrobustknowledge,
      title={Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization}, 
      author={Phillip Guo* and Aaquib Syed* and Abhay Sheshadri and Aidan Ewart and Gintare Karolina Dziugaite},
      year={2024},
      eprint={2410.12949},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      selected=True,
      note = {We find that high level understanding of model components involved in knowledge retrieval informs significantly more robust model editing without side effects. In comparison, previous automated localization approaches used for editing/unlearning are in a sense more precise but significantly less robust. :trophy: The preprint version of this paper received a spotlight at the ICML 2024 Mechanistic Interpretability Workshop.},
      url={https://arxiv.org/abs/2410.12949}, 
}

@misc{sheshadri2024targetedlatentadversarialtraining,
      title={Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs}, 
      author={Abhay Sheshadri* and Aidan Ewart* and Phillip Guo* and Aengus Lynch* and Cindy Wu* and Vivek Hebbar* and Henry Sleight and Asa Cooper Stickland and Ethan Perez and Dylan Hadfield-Menell and Stephen Casper},
      year={2024},
      eprint={2407.15549},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      selected=True,
      note = {We apply latent adversarial training to LLMs, allowing us to improve jailbreak robustness, remove backdoors (a la sleeper agents), and unlearn dangerous/unwanted knowledge robustly.},
      url={https://arxiv.org/abs/2407.15549}, 
      html={https://arxiv.org/abs/2407.15549}
}

@inproceedings{
guo2024robust,
title={Robust Unlearning via Mechanistic Localizations},
author={Phillip Huang Guo* and Aaquib Syed* and Abhay Sheshadri and Aidan Ewart and Gintare Karolina Dziugaite},
booktitle={ICML 2024 Workshop on Mechanistic Interpretability},
year={2024},
url={https://openreview.net/forum?id=06pNzrEjnH},
selected=False,
html={https://openreview.net/forum?id=06pNzrEjnH},
note = {:trophy: Selected as a spotlight! In this preprint, we find that high-level manual understanding of various model components in knowledge retrieval informs significantly more robust unlearning with fewer side effects. In comparison, previous automated interpretability and localization approaches used for editing/unlearning are in a sense more precise but significantly less robust. },
}

@misc{lynch2024methodsevaluaterobustunlearning,
      title={Eight Methods to Evaluate Robust Unlearning in LLMs}, 
      author={Aengus Lynch* and Phillip Guo* and Aidan Ewart* and Stephen Casper and Dylan Hadfield-Menell},
      year={2024},
      eprint={2402.16835},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16835},
      html={https://arxiv.org/abs/2402.16835}
}

@inproceedings{campbell2023localizinglyingllamaunderstanding,
      title={Localizing Lying in Llama: Understanding Instructed Dishonesty on True-False Questions Through Prompting, Probing, and Patching}, 
      author={James Campbell* and Richard Ren* and Phillip Guo*},
      year={2023},
      eprint={2311.15131},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.15131}, 
      booktitle={NeurIPS 2023 Workshop for Socially Responsible Language Modelling Research},
      html={https://arxiv.org/abs/2311.15131}
}

@misc{zou2023representationengineeringtopdownapproach,
      title={Representation Engineering: A Top-Down Approach to AI Transparency}, 
      author={Andy Zou and Long Phan* and Sarah Chen* and James Campbell* and Phillip Guo* and Richard Ren* and Alexander Pan and Xuwang Yin and Mantas Mazeika and Ann-Kathrin Dombrowski and Shashwat Goel and Nathaniel Li and Michael J. Byun and Zifan Wang and Alex Mallen and Steven Basart and Sanmi Koyejo and Dawn Song and Matt Fredrikson and J. Zico Kolter and Dan Hendrycks},
      year={2023},
      eprint={2310.01405},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.01405}, 
      selected=True,
      html={https://www.ai-transparency.org/},
      note={We characterize the area of Representation Engineering, an approach to practical interpretability that places population-level representations at the center of analysis. We gain traction on a range of safety-relevant problems by monitoring and manipulating high-level features of truthfulness, memorization, power-seeking, and more.}
}


@inproceedings{
syed2023prune,
author={Aaquib Syed* and Phillip Huang Guo* and Vijaykaarti Sundarapandiyan*},
booktitle={ICLR 2023 Tiny Papers Workshop},
title={Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models},
year={2023},
url={https://openreview.net/forum?id=cKlgcx7nSZ},
note = {:trophy: Top 5% of submitted papers, invited to present!},
html = {https://openreview.net/forum?id=cKlgcx7nSZ}
}

@INPROCEEDINGS{10015266,
  author={Guo, Phillip and Fu, Michael C.},
  booktitle={2022 Winter Simulation Conference (WSC)}, 
  title={Bandit-Based Multi-Start Strategies for Global Continuous Optimization}, 
  year={2022},
  volume={},
  number={},
  pages={3194-3205},
  keywords={Program processors;Computational modeling;Stochastic processes;Parallel processing;Search problems;Minimization;Resource management},
  doi={10.1109/WSC57314.2022.10015266},
  html={https://ieeexplore.ieee.org/document/10015266}
}